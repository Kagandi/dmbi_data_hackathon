{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "OUTLINE:\n",
    "    Predict missing Drug-drug interaction.\n",
    "    Nodes of DDI graph are embedded via skip-gram over 2nd order biased random walk.\n",
    "    The distance matrix is computed from cosine similarity of embedding vectors.\n",
    "    Per each pair of drugs, 2 digitized 2-D pseudo-CDFs (unnormalized) of distances to all other drugs are computed: 1 for all linked pairs, and 1 for all unlinked pairs.\n",
    "    Memberwise difference of this CDFs is flattened and used as input layer for binary classifiers (RF, XGB, Logistic Regression, KNN, Extreme Trees).\n",
    "    Iterative ensemble learning uplied to the top of these classifiers.\n",
    "    Trained on snapshots (2009 vs. 2012) of the DrugBank database, downloaded from http://seeslab.info/downloads/drug-drug-interactions/\n",
    "    Guimera, R, Sales-Pardo, M., \"A network inference method for large-scale unsupervised identification of novel drug-drug interactions\" PLOS Comput. Biol. 9 (12) , e1003374 (2013).\n",
    "    \n",
    "TO DO:\n",
    " - Vectorize/numba np.apply_along_axis(get_featv,...\n",
    " \n",
    "TO IMPROVE:\n",
    " - play with node2vec params (especially p, q). Quality of embedding could be remotely assessed by pharmacological similarity of neighbours/cluters of drugs as implied by resulted distances.\n",
    " - replace node2vec embedding by alternative (like VERSE, see https://github.com/chihming/awesome-network-embedding )\n",
    " - replace cosine similarity by alternatives\n",
    " - new features: for drugs d1 and e2 - concatenate Hadamard product of embeddings(d1) and avg(embeddings(closest_linked(d2, threshold2, d1, threshold1))) and reciprocal one. Here closest_linked(d2, threshold2, d1, threshold1) - drugs from threshold2-vicinity of d2 linked to drugs from threshold1-vicinity of d1 \n",
    " - new features: apply clustering (HDBSCAN or whatever), concatenate Hadamard product of embeddings(d1) and avg(embeddings(cluster_linked(d2, d1))) and reciprocal one. Here cluster_linked(d2, d1) - drugs from the cluster of d2 linked to drugs from the cluster of d1 \n",
    " - apply ensemble (say https://github.com/levyben/DeepSuperLearner) for alternative hyperparams of learners, embeddings, digitization etc.\n",
    " - top-level ensemble based on scores for (i,j) and (j,i). Input - min and max of these 2 scores. \n",
    " - n-fold validation. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install node2vec\n",
    "#!pip install git+https://github.com/levyben/DeepSuperLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "import scipy.sparse as sp\n",
    "from scipy.spatial.distance import cdist, pdist, squareform\n",
    "from scipy.stats.mstats import mquantiles\n",
    "#import qgrid\n",
    "from node2vec import Node2Vec\n",
    "import networkx as nx\n",
    "from itertools import islice\n",
    "from sklearn.ensemble.forest import ExtraTreesClassifier as ExtremeRandomizedTrees\n",
    "from sklearn.neighbors import KNeighborsClassifier as kNearestNeighbors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from deepSuperLearner import *\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import hdbscan\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_GT(filenameOldDDIList, filenameNewDDIList):\n",
    "    \"\"\"Prepare Ground Truth list of DDI\n",
    "    Parameters\n",
    "    ----------\n",
    "    filenameOldDDIList: file name of old (base) DDI List\n",
    "    filenameOldDDIList: file name of old (updated) DDI List\n",
    "    Returns\n",
    "    -------\n",
    "    m1: symmetric adjacency matrix of old DDI\n",
    "    edgesNew: list of new edges (each edge is a pair of DrugIDs)\n",
    "    lookupDrugName2ID: dictionary DrugName -> DrugID\n",
    "    \"\"\"\n",
    "    df1=pd.read_csv(filenameOldDDIList,sep=' ', header = None, names=['name1','name2'])\n",
    "    df2=pd.read_csv(filenameNewDDIList,sep=' ', header = None, names=['name1','name2'])\n",
    "    s1=set(df1.name1.unique()).union(set(df1.name2.unique()))\n",
    "    s2=set(df2.name1.unique()).union(set(df2.name2.unique()))\n",
    "    s=s1.union(s2)\n",
    "    lookupDrugName2ID = {value: key for (key, value) in enumerate(sorted(s))}\n",
    "    df1['d1']=df1.name1.replace(lookupDrugName2ID)\n",
    "    df1['d2']=df1.name2.replace(lookupDrugName2ID)\n",
    "    df2['d1']=df2.name1.replace(lookupDrugName2ID)\n",
    "    df2['d2']=df2.name2.replace(lookupDrugName2ID)\n",
    "    s1n=set(df1.d1.unique()).union(set(df1.d2.unique()))\n",
    "    s2n=set(df2.d1.unique()).union(set(df2.d2.unique()))\n",
    "    onlyIn1 = np.array(sorted(list(s1n-s2n))) # drugs excluded (or renamed) in new DDI List \n",
    "    onlyIn2 = np.array(sorted(list(s2n-s1n))) # drugs added (or renamed) in new DDI List \n",
    "    d1n=df1.d1.values\n",
    "    d2n=df1.d2.values\n",
    "    m1 = coo_matrix((np.ones(2*df1.shape[0],'int'), (np.hstack([d1n,d2n]),np.hstack([d2n,d1n]))))\n",
    "    m1 = ((m1 + m1.transpose())>0).astype('int').tolil() # adjacency DDI matrix, old DDI List\n",
    "\n",
    "    d1n=df2.d1.values\n",
    "    d2n=df2.d2.values\n",
    "    m2 = coo_matrix((np.ones(2*df2.shape[0],'int'), (np.hstack([d1n,d2n]),np.hstack([d2n,d1n]))))\n",
    "    m2 = ((m2 + m2.transpose())>0).astype('int').tolil() # adjacency DDI matrix, new DDI List\n",
    "\n",
    "    #jac12=cdist(m1[onlyIn1,:].toarray(),m2[onlyIn2,:].toarray(),'jaccard')\n",
    "    #dfSimilar=pd.DataFrame.from_records(\n",
    "    #    [[jac12[i,j],onlyIn1[i],onlyIn2[j],d_r[onlyIn1[i]],d_r[onlyIn2[j]]] for (i,j) in enumerate (np.argmin(jac12,1)) ],\n",
    "    #    columns=['coeff','i1','i2','name1','name2']\n",
    "    #)\n",
    "    #dfSimilar.to_csv('similar.csv')\n",
    "    dfIdentical=pd.DataFrame.from_records([\n",
    "                ('glibenclamide','glyburide'),\n",
    "                ('demecarium_bromide','demecarium'),\n",
    "                ('ipratropium','ipratropium_bromide'),\n",
    "                ('marinol','dronabinol'),\n",
    "                ('echothiophate_iodide','echothiophate'),\n",
    "                ('doxacurium','doxacurium_chloride'),\n",
    "                ('aspirin','acetylsalicylic_acid'),\n",
    "                ('salicylate-magnesium','magnesium_salicylate'),\n",
    "                ('insulin-detemir','insulin_detemir')\n",
    "            ],\n",
    "            columns=['iOld','iNew'])\n",
    "    dfIdentical.iOld.replace(lookupDrugName2ID,inplace=True)\n",
    "    dfIdentical.iNew.replace(lookupDrugName2ID,inplace=True)\n",
    "    \n",
    "    # drugs renamed in new DDI List - keep the old names\n",
    "    m2[dfIdentical.iOld,:]=m2[dfIdentical.iNew,:]\n",
    "    m2[:,dfIdentical.iOld]=m2[:,dfIdentical.iNew]\n",
    "    m1_orig=m1.copy()\n",
    "    m2_orig=m2.copy()\n",
    "    \n",
    "    # drugs added in new DDI List - ignore them\n",
    "    m2[onlyIn2,:]=0\n",
    "    m2[:,onlyIn2]=0\n",
    "    \n",
    "    # drugs excluded in new DDI List - ignore them\n",
    "    onlyIn1_noIdentical = list(set(onlyIn1)-set(dfIdentical.iOld))\n",
    "    m1[onlyIn1_noIdentical,:]=0\n",
    "    m1[:,onlyIn1_noIdentical]=0\n",
    "    \n",
    "    # all interactions of valid drugs in old DDI List\n",
    "    sedg1 = set(zip(*m1.nonzero()))\n",
    "\n",
    "    # all interactions of valid drugs in new DDI List\n",
    "    sedg2 = set(zip(*m2.nonzero()))\n",
    "    \n",
    "    # interactions excluded in new DDI List - ignore them\n",
    "    edgesObsolete=np.array(list(sedg1-sedg2))\n",
    "    for e in edgesObsolete:\n",
    "        m1[e[0],e[1]] = 0\n",
    "        \n",
    "    # squeeze: exclude drugs without valid interactions\n",
    "    isValidDrugID=(np.diff(m1.tocsr().indptr) != 0)\n",
    "    newDrugIDs=np.repeat(-1,m1.shape[0])\n",
    "    newDrugIDs[isValidDrugID]=np.arange(isValidDrugID.sum())\n",
    "    m1=m1[isValidDrugID,:][:,isValidDrugID]\n",
    "    lookupDrugName2ID =  {k : newDrugIDs[v] for (k,v) in lookupDrugName2ID.items() if newDrugIDs[v]>=0}\n",
    "    \n",
    "    # interactions added in new DDI List - keep them as training set\n",
    "    edgesNew=[ [newDrugIDs[a],newDrugIDs[b]]  \n",
    "              for (a,b) in sedg2-sedg1\n",
    "              if b>a and newDrugIDs[a]>=0 and newDrugIDs[b]>=0]\n",
    "    edgesNew = np.unique(edgesNew, axis=0)\n",
    "    return (m1, edgesNew, lookupDrugName2ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.92 s, sys: 36 ms, total: 2.96 s\n",
      "Wall time: 2.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "(m1, edgesNew, lookupDrugName2ID) = prepare_GT('drugbank_200901.dat', 'drugbank_201204.dat')\n",
    "# reverse lookup:  DrugID -> DrugName\n",
    "lookupDrugID2Name = np.array(sorted(lookupDrugName2ID, key=lookupDrugName2ID.get))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 4.77 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# SAVE\n",
    "def save_DDI_matrix(fnameOldEdges, fnameDrugID2Name, m1, lookupDrugID2Name):\n",
    "    np.savetxt(fnameDrugID2Name,lookupDrugID2Name,fmt='%s')\n",
    "    np.savetxt(fnameOldEdges,np.array(list(zip(*m1.nonzero()))),fmt='%d',delimiter=',')\n",
    "#save_DDI_matrix(\"old_edges.csv\", \"lookupDrugID2Name.csv\", m1, lookupDrugID2Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52 ms, sys: 4 ms, total: 56 ms\n",
      "Wall time: 59.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# LOAD\n",
    "def load_DDI_matrix(fnameOldEdges, fnameDrugID2Name):\n",
    "    lookupDrugID2Name = np.genfromtxt(fnameDrugID2Name,dtype=np.unicode_)\n",
    "    lookupDrugName2ID = {v:i for i,v in enumerate(lookupDrugID2Name)}\n",
    "    data_ = np.genfromtxt(fnameOldEdges,delimiter=',',dtype='int')\n",
    "    m1 = sp.coo_matrix((np.ones(data_.shape[0],int),(data_[:,0],data_[:,1]))).tolil()\n",
    "    return (m1, lookupDrugID2Name, lookupDrugName2ID)\n",
    "\n",
    "m1, lookupDrugID2Name, lookupDrugName2ID = load_DDI_matrix(\"old_edges.csv\", \"lookupDrugID2Name.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 356 ms, sys: 72 ms, total: 428 ms\n",
      "Wall time: 352 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Save Ground Trith - New Links and Non-Links\n",
    "OVERSAMPLE_NEWLINKS = 5 # how much New Liks to obe oversampled vs. Non-Links\n",
    "def get_nonlink(m1,edgesNew):\n",
    "    \"\"\"Generator of non-link pairs\n",
    "    Parameters\n",
    "    ----------\n",
    "    m1: adjacency matrix of old DDI\n",
    "    edgesNew: list of new edges, assume to contain distinct edges\n",
    "    Returns\n",
    "    -------\n",
    "    non_link: pair of non-linked node IDs\n",
    "    \"\"\"\n",
    "    n_validIDs=m1.shape[0]\n",
    "    while True:\n",
    "        i = np.random.randint(n_validIDs-1)\n",
    "        j = i + 1 + np.random.randint(n_validIDs - i - 1)\n",
    "        if m1[i,j]:\n",
    "            continue\n",
    "        edge = [i,j]\n",
    "        if edge in edgesNew:\n",
    "            continue\n",
    "        yield edge\n",
    "nonEdges = np.array(list(islice (get_nonlink(m1,edgesNew),len(edgesNew)*OVERSAMPLE_NEWLINKS)))\n",
    "#np.savetxt('GT_nonlinks.csv', nonEdges, fmt='%d',delimiter=',')\n",
    "#np.savetxt('GT_links.csv', edgesNew, fmt='%d',delimiter=',') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_node2vec(m1,dimensions=64, p=1, q=1, walk_length=10, num_walks=1000, window=10, workers=8):\n",
    "    G=nx.from_scipy_sparse_matrix(sp.triu(m1,1))\n",
    "    node2vec = Node2Vec(G, dimensions=dimensions, p=p, q=q, walk_length=walk_length, num_walks=num_walks, workers=workers) \n",
    "    model = node2vec.fit(window=window, min_count=1, batch_words=4) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 1020/1020 [00:06<00:00, 152.57it/s]\n",
      "Generating walks (CPU: 5):  82%|████████▏ | 9/11 [00:02<00:00,  3.07it/s]]\n",
      "Generating walks (CPU: 14):  27%|██▋       | 3/11 [00:00<00:01,  4.29it/s]\n",
      "Generating walks (CPU: 7):  82%|████████▏ | 9/11 [00:02<00:00,  2.68it/s]]\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]██▏ | 9/11 [00:02<00:00,  2.70it/s]]\n",
      "Generating walks (CPU: 6): 100%|██████████| 11/11 [00:03<00:00,  2.54it/s]\n",
      "Generating walks (CPU: 10):  82%|████████▏ | 9/11 [00:02<00:00,  2.85it/s]\n",
      "Generating walks (CPU: 13):  73%|███████▎  | 8/11 [00:02<00:01,  2.87it/s]\n",
      "Generating walks (CPU: 12):  82%|████████▏ | 9/11 [00:02<00:00,  2.80it/s]]\n",
      "Generating walks (CPU: 19):  45%|████▌     | 5/11 [00:01<00:01,  3.45it/s]]\n",
      "Generating walks (CPU: 21):  36%|███▋      | 4/11 [00:01<00:01,  3.92it/s]]\n",
      "Generating walks (CPU: 14):  91%|█████████ | 10/11 [00:03<00:00,  2.94it/s]\n",
      "Generating walks (CPU: 23):  36%|███▋      | 4/11 [00:01<00:01,  3.93it/s]]\n",
      "Generating walks (CPU: 15): 100%|██████████| 11/11 [00:03<00:00,  2.92it/s]\n",
      "Generating walks (CPU: 25):  27%|██▋       | 3/11 [00:00<00:01,  4.52it/s]\n",
      "Generating walks (CPU: 20):  73%|███████▎  | 8/11 [00:02<00:00,  3.11it/s]]\n",
      "Generating walks (CPU: 28):  18%|█▊        | 2/11 [00:00<00:01,  5.81it/s]]\n",
      "Generating walks (CPU: 22):  73%|███████▎  | 8/11 [00:02<00:00,  3.14it/s]]\n",
      "Generating walks (CPU: 29):  27%|██▋       | 3/11 [00:00<00:01,  4.67it/s]]\n",
      "Generating walks (CPU: 26):  55%|█████▍    | 6/11 [00:01<00:01,  3.47it/s]]\n",
      "Generating walks (CPU: 23):  91%|█████████ | 10/11 [00:03<00:00,  2.77it/s]\n",
      "Generating walks (CPU: 27):  64%|██████▎   | 7/11 [00:02<00:01,  3.07it/s]]\n",
      "Generating walks (CPU: 34):  18%|█▊        | 2/11 [00:00<00:01,  6.17it/s]]\n",
      "Generating walks (CPU: 29):  64%|██████▎   | 7/11 [00:02<00:01,  3.22it/s]]\n",
      "Generating walks (CPU: 36):  18%|█▊        | 2/11 [00:00<00:01,  6.06it/s]]\n",
      "Generating walks (CPU: 34):  45%|████▌     | 5/11 [00:01<00:01,  3.64it/s]]\n",
      "Generating walks (CPU: 38):  18%|█▊        | 2/11 [00:00<00:01,  6.02it/s]]\n",
      "Generating walks (CPU: 33):  64%|██████▎   | 7/11 [00:02<00:01,  3.25it/s]]\n",
      "Generating walks (CPU: 40):  18%|█▊        | 2/11 [00:00<00:01,  6.04it/s]]\n",
      "Generating walks (CPU: 33):  82%|████████▏ | 9/11 [00:02<00:00,  2.94it/s]]\n",
      "Generating walks (CPU: 34):  82%|████████▏ | 9/11 [00:02<00:00,  3.12it/s]]\n",
      "Generating walks (CPU: 36):  73%|███████▎  | 8/11 [00:02<00:00,  3.09it/s]]\n",
      "Generating walks (CPU: 42):  40%|████      | 4/10 [00:00<00:01,  4.03it/s]]\n",
      "Generating walks (CPU: 43):  40%|████      | 4/10 [00:00<00:01,  4.07it/s]]\n",
      "Generating walks (CPU: 45):  30%|███       | 3/10 [00:00<00:01,  4.69it/s]]\n",
      "Generating walks (CPU: 48):   0%|          | 0/10 [00:00<?, ?it/s]3.02it/s]\n",
      "Generating walks (CPU: 42):  70%|███████   | 7/10 [00:02<00:00,  3.07it/s]]\n",
      "Generating walks (CPU: 42):  80%|████████  | 8/10 [00:02<00:00,  3.06it/s]]\n",
      "Generating walks (CPU: 46):  60%|██████    | 6/10 [00:01<00:01,  3.25it/s]]\n",
      "Generating walks (CPU: 41): 100%|██████████| 10/10 [00:03<00:00,  2.57it/s]\n",
      "Generating walks (CPU: 51):  30%|███       | 3/10 [00:00<00:01,  4.51it/s]]\n",
      "Generating walks (CPU: 46):  80%|████████  | 8/10 [00:02<00:00,  3.07it/s]]\n",
      "\n",
      "Generating walks (CPU: 53):  30%|███       | 3/10 [00:00<00:01,  4.72it/s]]\n",
      "Generating walks (CPU: 54):  30%|███       | 3/10 [00:00<00:01,  4.70it/s]]\n",
      "Generating walks (CPU: 55):  30%|███       | 3/10 [00:00<00:01,  4.71it/s]]\n",
      "Generating walks (CPU: 56):  30%|███       | 3/10 [00:00<00:01,  4.66it/s]]\n",
      "Generating walks (CPU: 58):  20%|██        | 2/10 [00:00<00:01,  5.91it/s]\n",
      "Generating walks (CPU: 51):  90%|█████████ | 9/10 [00:02<00:00,  2.91it/s]]\n",
      "Generating walks (CPU: 57):  50%|█████     | 5/10 [00:01<00:01,  3.46it/s]]\n",
      "Generating walks (CPU: 60):  30%|███       | 3/10 [00:00<00:01,  4.65it/s]]\n",
      "Generating walks (CPU: 55):  80%|████████  | 8/10 [00:02<00:00,  3.15it/s]]\n",
      "Generating walks (CPU: 63):  20%|██        | 2/10 [00:00<00:01,  6.05it/s]]\n",
      "Generating walks (CPU: 60):  50%|█████     | 5/10 [00:01<00:01,  3.41it/s]]\n",
      "Generating walks (CPU: 61):  50%|█████     | 5/10 [00:01<00:01,  3.49it/s]]\n",
      "Generating walks (CPU: 66):  20%|██        | 2/10 [00:00<00:01,  6.18it/s]]\n",
      "Generating walks (CPU: 59):  80%|████████  | 8/10 [00:02<00:00,  3.01it/s]]\n",
      "Generating walks (CPU: 67):  30%|███       | 3/10 [00:00<00:01,  4.52it/s]]\n",
      "Generating walks (CPU: 68):  30%|███       | 3/10 [00:00<00:01,  4.48it/s]\n",
      "Generating walks (CPU: 63):  80%|████████  | 8/10 [00:02<00:00,  3.04it/s]]\n",
      "Generating walks (CPU: 67):  60%|██████    | 6/10 [00:01<00:01,  3.42it/s]]\n",
      "Generating walks (CPU: 73):  20%|██        | 2/10 [00:00<00:01,  6.19it/s]]\n",
      "Generating walks (CPU: 74):  20%|██        | 2/10 [00:00<00:01,  6.07it/s]]\n",
      "Generating walks (CPU: 73):  40%|████      | 4/10 [00:00<00:01,  4.06it/s]]\n",
      "Generating walks (CPU: 71):  60%|██████    | 6/10 [00:01<00:01,  3.45it/s]\n",
      "Generating walks (CPU: 74):  40%|████      | 4/10 [00:00<00:01,  4.03it/s]]\n",
      "Generating walks (CPU: 67): 100%|██████████| 10/10 [00:03<00:00,  2.93it/s]\n",
      "Generating walks (CPU: 74):  60%|██████    | 6/10 [00:01<00:01,  3.48it/s]]\n",
      "Generating walks (CPU: 79):  20%|██        | 2/10 [00:00<00:01,  6.16it/s]]\n",
      "Generating walks (CPU: 81):   0%|          | 0/10 [00:00<?, ?it/s].32it/s]\n",
      "Generating walks (CPU: 77):  50%|█████     | 5/10 [00:01<00:01,  3.52it/s]]\n",
      "Generating walks (CPU: 76):  70%|███████   | 7/10 [00:02<00:00,  3.07it/s]]\n",
      "Generating walks (CPU: 73): 100%|██████████| 10/10 [00:03<00:00,  2.84it/s]\n",
      "Generating walks (CPU: 79):  60%|██████    | 6/10 [00:01<00:01,  3.46it/s]]\n",
      "Generating walks (CPU: 75): 100%|██████████| 10/10 [00:03<00:00,  3.06it/s]\n",
      "Generating walks (CPU: 84):  40%|████      | 4/10 [00:01<00:01,  3.94it/s]]\n",
      "Generating walks (CPU: 82):  60%|██████    | 6/10 [00:01<00:01,  3.30it/s]\n",
      "Generating walks (CPU: 87):  40%|████      | 4/10 [00:01<00:01,  3.80it/s]]\n",
      "Generating walks (CPU: 82):  80%|████████  | 8/10 [00:02<00:00,  3.18it/s]\n",
      "Generating walks (CPU: 81):  90%|█████████ | 9/10 [00:02<00:00,  2.92it/s]\n",
      "Generating walks (CPU: 91):  30%|███       | 3/10 [00:00<00:01,  4.55it/s]]\n",
      "Generating walks (CPU: 91):  40%|████      | 4/10 [00:01<00:01,  3.96it/s]]\n",
      "Generating walks (CPU: 94):  20%|██        | 2/10 [00:00<00:01,  6.08it/s]]\n",
      "Generating walks (CPU: 87):  80%|████████  | 8/10 [00:02<00:00,  3.09it/s]]\n",
      "Generating walks (CPU: 89):  70%|███████   | 7/10 [00:02<00:00,  3.20it/s]]\n",
      "Generating walks (CPU: 86): 100%|██████████| 10/10 [00:03<00:00,  2.97it/s]\n",
      "Generating walks (CPU: 94):  50%|█████     | 5/10 [00:01<00:01,  3.65it/s]]\n",
      "Generating walks (CPU: 90):  90%|█████████ | 9/10 [00:02<00:00,  3.17it/s]]\n",
      "Generating walks (CPU: 89): 100%|██████████| 10/10 [00:03<00:00,  3.00it/s]\n",
      "Generating walks (CPU: 92):  90%|█████████ | 9/10 [00:02<00:00,  3.13it/s]]\n",
      "Generating walks (CPU: 91): 100%|██████████| 10/10 [00:03<00:00,  3.00it/s]\n",
      "Generating walks (CPU: 92): 100%|██████████| 10/10 [00:03<00:00,  2.97it/s]\n",
      "Generating walks (CPU: 93): 100%|██████████| 10/10 [00:03<00:00,  3.02it/s]\n",
      "Generating walks (CPU: 94): 100%|██████████| 10/10 [00:03<00:00,  2.93it/s]\n",
      "Generating walks (CPU: 95): 100%|██████████| 10/10 [00:03<00:00,  3.06it/s]\n",
      "Generating walks (CPU: 96): 100%|██████████| 10/10 [00:03<00:00,  3.07it/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#model = get_node2vec(m1,dimensions=64, p=1, q=1, walk_length=10, num_walks=1000, window=10, workers=96)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.wv.save_word2vec_format(\"node2vec-embeddings_64_1_1_10_1000_10.w2v\")\n",
    "#model.save(\"node2vec-model_64_1_1_10_1000_10.w2v\")\n",
    "#model1.wv.save_word2vec_format(\"node2vec-embeddings_64_1_1_30_200_10.w2v\")\n",
    "#model1.save(\"node2vec-model_64_1_1_30_200_10.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wv_toembed(wv):\n",
    "    \"\"\"Convert gensim Word2Vec embeddings to numpy array\n",
    "    Parameters\n",
    "    ----------\n",
    "    wv: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    Returns\n",
    "    -------\n",
    "    embeddings: 2-D ndarray of shape (N_WORDS, N_EMBEDDING_DIMS)\n",
    "    ks: 1-D ndarray of represented \"words\" (vertices)\n",
    "    Embeddidng vectors of non-represented \"words\" (vertices) are zero\n",
    "    \"\"\"\n",
    "    (Nwords,Ndims) = wv.vectors.shape\n",
    "    ks = [int(x) for x in wv.vocab.keys()]\n",
    "    idxs = [v.index for (k,v) in wv.vocab.items()]\n",
    "    row_ind = np.repeat(ks,Ndims)\n",
    "    col_ind = [x for x in range(Ndims)]*Nwords\n",
    "    embeddings = sp.csr_matrix((wv.vectors[idxs,:].reshape(-1), (row_ind, col_ind)), (np.max(ks)+1,Ndims))\n",
    "    return (embeddings.toarray(),ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
    }
   ],
   "source": [
    "embeddings, ks = wv_toembed(model.wv)\n",
    "dist=squareform(pdist(embeddings,'cosine'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dist_bins(v, analog_ratio = 0.04, n_bin_edges = 10):\n",
    "    \"\"\"Get adaptive bins for histogram of drug-drug distances, per each drug\n",
    "    Parameters\n",
    "    ----------\n",
    "    dist: 1-D ndarray - vector of drug-drug distances\n",
    "    analog_ratio: ratio of to be taken take into account as \"near enough\"\n",
    "    n_bin_edges: number of bins to digitize distances \n",
    "    Returns\n",
    "    -------\n",
    "    bins: 1-D ndarray of bins (1, n_bin_edges+1)\n",
    "    \"\"\"\n",
    "    bins = mquantiles(v,\n",
    "                        prob=np.linspace(0,analog_ratio,n_bin_edges),\n",
    "                        alphap=3/8, betap=3/8) #Blom. The resulting quantile estimates are approximately unbiased if dist is normally distributed\n",
    "    return np.hstack((bins,np.inf))\n",
    "# adaptive bins for histogram of drug-drug distances, per each drug\n",
    "adaptive_bins = np.apply_along_axis(get_dist_bins, 1, dist)\n",
    "# global bins for histogram of drug-drug distances, the same for each drug\n",
    "global_bins = np.tile(get_dist_bins(dist),(dist.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iLink,jLink=m1.nonzero()\n",
    "iNoLink,jNoLink=np.where(m1.A==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cdf2_feat(dist, i, j, iLink, jLink, iNoLink, jNoLink, bins):\n",
    "    \"\"\"For a pair of drugs, get features (as matrix) based on 2-D CDF matching, \n",
    "    Parameters\n",
    "    ----------\n",
    "    dist: 2-D ndarray - symmetric matrix of drug-drug distances\n",
    "    i, j: IDs (scalars) of drugs in question\n",
    "    iLink, jLink: IDs (vectors) of all drugs with DDI (edge between them) \n",
    "    iNoLink, jNoLink: IDs (vectors) of all drugs without DDI (edge between them) \n",
    "    bins: 2-D ndarray digitization bins\n",
    "    Returns\n",
    "    -------\n",
    "    D: 2-D ndarray of features: \"Hellinger-like\" matching of 2-D empirical CDNs of distances \n",
    "    \"\"\"\n",
    "    DLink=np.histogramdd(np.vstack((dist[i,jLink],dist[iLink,j])).T,bins=(bins[i],bins[j]))[0].astype('int')\n",
    "    DNoLink=np.histogramdd(np.vstack((dist[i,jNoLink],dist[iNoLink,j])).T,bins=(bins[i],bins[j]))[0].astype('int')\n",
    "    return np.sqrt(DLink) - np.sqrt(DNoLink) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_featv(ij):\n",
    "    \"\"\"For a pair of drugs, get feature vector\n",
    "    Parameters\n",
    "    ----------\n",
    "    ij: pair of IDs (scalars) of drugs in question\n",
    "    Returns\n",
    "    -------\n",
    "    D: 1-D ndarray of features\n",
    "    \"\"\"\n",
    "    i,j = ij\n",
    "    f1=get_cdf2_feat(dist, i, j, iLink, jLink, iNoLink, jNoLink, adaptive_bins).flatten()\n",
    "    f2=get_cdf2_feat(dist, i, j, iLink, jLink, iNoLink, jNoLink, global_bins).flatten()\n",
    "    return np.concatenate((f1, f2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_edges_and_labels(fname, label, test_size=0.2):\n",
    "    D = np.genfromtxt(fname,delimiter=',',dtype='int')\n",
    "    Y = np.repeat(label, D.shape[0])\n",
    "    e_train, e_test, y_train, y_test = train_test_split(D, Y, shuffle=False, test_size=test_size)\n",
    "    e_train = np.vstack((e_train,np.apply_along_axis(lambda x: [x[1],x[0]], 1, e_train)))\n",
    "    e_test = np.vstack((e_test,np.apply_along_axis(lambda x: [x[1],x[0]], 1, e_test)))\n",
    "    y_train = np.tile(y_train,2)\n",
    "    y_test = np.tile(y_test,2)\n",
    "    return (e_train, e_test, y_train, y_test)\n",
    "\n",
    "e_train_nonlinks, e_test_nonlinks, y_train_nonlinks, y_test_nonlinks = get_edges_and_labels('GT_nonlinks.csv', label=0)\n",
    "e_train_links, e_test_links, y_train_links, y_test_links = get_edges_and_labels('GT_links.csv', label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#X_test_links = np.apply_along_axis(get_featv, 1, e_test_links)\n",
    "#X_train_links = np.apply_along_axis(get_featv, 1, e_train_links)\n",
    "#X_test_nonlinks = np.apply_along_axis(get_featv, 1, e_test_nonlinks)\n",
    "#X_train_nonlinks = np.apply_along_axis(get_featv, 1, e_train_nonlinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "YeX_train = np.vstack((\n",
    "\t    np.concatenate((y_train_nonlinks.reshape(-1,1),e_train_nonlinks,X_train_nonlinks),1),\n",
    "\t\tnp.tile(np.concatenate((y_train_links.reshape(-1,1),e_train_links,X_train_links),1),(OVERSAMPLE_NEWLINKS,1))\n",
    "\t))\n",
    "np.random.shuffle(YeX_train)\n",
    "YeX_test = np.vstack((\n",
    "\t    np.concatenate((y_test_nonlinks.reshape(-1,1),e_test_nonlinks,X_test_nonlinks),1),\n",
    "\t\tnp.tile(np.concatenate((y_test_links.reshape(-1,1),e_test_links,X_test_links),1),(OVERSAMPLE_NEWLINKS,1))\n",
    "\t))\n",
    "np.random.shuffle(YeX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SAVE\n",
    "np.savez(\"YeX_train.npz\", YeX_train)\n",
    "np.savez(\"YeX_test.npz\", YeX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "YeX_train=np.load(\"YeX_train.npz\")['arr_0']\n",
    "YeX_test=np.load(\"YeX_test.npz\")['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_test = YeX_test[:,0]\n",
    "e_test = YeX_test[:,1:3]\n",
    "X_test = YeX_test[:,3:]\n",
    "Y_train = YeX_train[:,0]\n",
    "e_train = YeX_train[:,1:3]\n",
    "X_train = YeX_train[:,3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERT_learner = ExtremeRandomizedTrees(n_estimators=200, max_depth=None, max_features=15)\n",
    "kNN_learner = kNearestNeighbors(n_neighbors=11)\n",
    "LR_learner = LogisticRegression(penalty='l2', C=1)\n",
    "LR_learnerL1 = LogisticRegression(penalty='l1', C=0.1)\n",
    "RFC_learner = RandomForestClassifier(n_estimators=200, max_depth=None)\n",
    "XGB_learner = XGBClassifier(n_estimators=200, max_depth=10, learning_rate=0.01)\n",
    "Base_learners = {'ExtremeRandomizedTrees':ERT_learner, 'kNearestNeighbors':kNN_learner, 'LogisticRegression':LR_learner, 'LogisticRegressionL1':LR_learnerL1,\n",
    "                     'RandomForestClassifier':RFC_learner, 'XGBClassifier':XGB_learner}\n",
    "DSL_learner = DeepSuperLearner(Base_learners)\n",
    "DSL_learner.fit(X_train, Y_train,max_iterations=50,sample_weight=None)\n",
    "DSL_learner.get_precision_recall(X_test, Y_test, show_graphs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
